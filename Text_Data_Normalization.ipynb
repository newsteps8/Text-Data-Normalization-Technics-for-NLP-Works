{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0t_g0kY4SPMi"
   },
   "outputs": [],
   "source": [
    "datafile = '/home/busra/PycharmProjects/Piton/myenv/bulk.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NfdjH9scukOr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting turkishnlp\n",
      "  Downloading turkishnlp-0.0.61.tar.gz (8.4 kB)\n",
      "Building wheels for collected packages: turkishnlp\n",
      "  Building wheel for turkishnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for turkishnlp: filename=turkishnlp-0.0.61-py3-none-any.whl size=7556 sha256=e87c19b46960bd4634cbf42e184e399057e54cc1b27d0e4de2d3e5e2e124bb70\n",
      "  Stored in directory: /home/busra/.cache/pip/wheels/94/c1/10/7c6af923624747c63d9d74a7da25525e65ee4d76dddebb8db7\n",
      "Successfully built turkishnlp\n",
      "Installing collected packages: turkishnlp\n",
      "Successfully installed turkishnlp-0.0.61\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/busra/PycharmProjects/Piton/myenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install turkishnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qTlAaiHzSPVe"
   },
   "outputs": [],
   "source": [
    "#text processing\n",
    "import re\n",
    "import string\n",
    "from gensim import corpora, models, similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qk2Ms8KPzIUm"
   },
   "source": [
    "*Remove Emoji*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HvoRw5bOSPY2"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxHuJJIwzP5I"
   },
   "source": [
    "Prepare Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yGmt0MmJSPTZ"
   },
   "outputs": [],
   "source": [
    "with open('stopwords-tr.txt', 'r') as f:\n",
    "    myList = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QUtW6v9zYjS"
   },
   "source": [
    "Remove URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6abBhohRSPRQ"
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P001O_D5zfo0"
   },
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jD7KUtRaSPLn"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "noktalama_isaretleri = string.punctuation\n",
    "def noktalama_temizleme(text):\n",
    "    return text.translate(str.maketrans('', '', noktalama_isaretleri))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "calnhLZqzoMl"
   },
   "source": [
    "Seperate Sentence to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "equ3SiSSSPKE"
   },
   "outputs": [],
   "source": [
    "def word_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): any sentence.\n",
    "    Returns:\n",
    "        list: each item is a word.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    acronym_each_dot = r\"(?:[a-zğçşöüı]\\.){2,}\"\n",
    "    acronym_end_dot = r\"\\b[a-zğçşöüı]{2,3}\\.\"\n",
    "    suffixes = r\"[a-zğçşöüı]{3,}' ?[a-zğçşöüı]{0,3}\"\n",
    "    numbers = r\"\\d+[.,:\\d]+\"\n",
    "    any_word = r\"[a-zğçşöüı]+\"\n",
    "    punctuations = r\"[a-zğçşöüı]*[.,!?;:]\"\n",
    "    word_regex = \"|\".join([acronym_each_dot,\n",
    "                           acronym_end_dot,\n",
    "                           suffixes,\n",
    "                           numbers,\n",
    "                           any_word,\n",
    "                           punctuations])\n",
    "    \n",
    "    sentence = re.compile(\"%s\"%word_regex, re.I).findall(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCSWn0sJzvpl"
   },
   "source": [
    "Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Sj7yxciaSPIC"
   },
   "outputs": [],
   "source": [
    "def initial_lower(text):\n",
    "    \"\"\"\n",
    "    Function to clean text-remove punctuations, lowercase text etc.    \n",
    "    \"\"\"   \n",
    "    text = text.lower() # lower case text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgwOi6bJz416"
   },
   "source": [
    "*Remove Stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qXzHbeO1SPEF"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = myList\n",
    "    mytokens = text.split(' ')\n",
    "    mytokens = [word for word in mytokens if word not in stop_words]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install turkishnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yH6rwOk2u8l5"
   },
   "outputs": [],
   "source": [
    "import turkishnlp\n",
    "from turkishnlp import detector\n",
    "obj = detector.TurkishNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check if the text is in Turkish*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import turkishnlp\n",
    "from turkishnlp import detector\n",
    "obj = detector.TurkishNLP()\n",
    "\n",
    "obj.download()\n",
    "obj.create_word_set()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#read the csv file \n",
    "datafile = '/home/busra/PycharmProjects/Piton/myenv/bulk.csv'\n",
    "df = pd.read_csv(datafile,error_bad_lines=False)\n",
    "\n",
    "def isTurkish(text):\n",
    "\n",
    "    try:\n",
    "        if type(text) == str:\n",
    "            obj.is_turkish(text)\n",
    "            if str(obj.is_turkish(text))==\"True\":\n",
    "                return text\n",
    "            else:\n",
    "                return float(\"NaN\")\n",
    "    except ZeroDivisionError :\n",
    "        text=float(\"NaN\")\n",
    "        return text\n",
    "df['MESSAGE'] = df['MESSAGE'].apply(isTurkish) # noisy text data ->>>> normalized text data\n",
    "df.to_csv('example2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTsZLwiT0wMe"
   },
   "source": [
    "*Read CSV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yeUG2dhIlpG7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#read the csv file \n",
    "df = pd.read_csv(datafile,error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8WtuIdn0nciB"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xSFexssEmH0j"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset = [\"MESSAGE\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "b0ijaCBFjooH",
    "outputId": "b8c5e3dd-5be5-4360-c765-7c6acc42e025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 35357 texts: 0.004491472244262695 min\n"
     ]
    }
   ],
   "source": [
    "import time   \n",
    "t1 = time.time()   \n",
    "df['MESSAGE'] = df['MESSAGE'].apply(noktalama_temizleme)\n",
    "df['MESSAGE'] = df['MESSAGE'].apply(remove_urls)\n",
    "df['MESSAGE'] = df['MESSAGE'].apply(remove_emoji)\n",
    "t2 = time.time()  \n",
    "print(\"Time to clean and tokenize\", len(df), \"texts:\", (t2-t1)/60, \"min\") #Time to clean and tokenize 3209 reviews: 0.21254388093948365 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XdaMX6UUCuDx"
   },
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "#Convert NaN values to empty string\n",
    "\n",
    "df.replace(' ', nan_value, inplace=True)\n",
    "\n",
    "df.dropna(subset = [\"MESSAGE\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8ygjg8aq9VOP",
    "outputId": "23e11b99-3536-4f19-cc9b-f44ea38382c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34729, 14)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jN4H_bVo3T1g",
    "outputId": "3fe26b86-59f9-45d1-db2b-23aab5399b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 21266 texts: 0.08135783672332764 min\n"
     ]
    }
   ],
   "source": [
    "import time   \n",
    "t1 = time.time()   \n",
    "df['MESSAGE'] = df['MESSAGE'].apply(initial_lower)\n",
    "df['MESSAGE'] = df['MESSAGE'].apply(remove_stop_words)\n",
    "t2 = time.time()  \n",
    "print(\"Time to clean and tokenize\", len(df), \"texts:\", (t2-t1)/60, \"min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eguia8MfO8bp"
   },
   "outputs": [],
   "source": [
    "df['tokenized_texts'] = df['MESSAGE'].apply(word_tokenize) #noisy text data ->>>> normalized text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_YSKU9vqmVsW"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BulkDataClusturing_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
